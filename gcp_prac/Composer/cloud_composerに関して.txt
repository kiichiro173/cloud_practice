Airflowに関して
→https://zenn.dev/momota/articles/4634b949cad467
→https://zenn.dev/akimen/articles/e67fd89e9f1e0d
→DAG：依存か安慶のある処理同士をつなぎ込んだもの

CloudComposerはApache Airflowが活用されている。


以下実装
ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
Cloud Composerの環境を作成
$ gcloud composer environments create gcpbook-ch6 --location us-central1
gcpbook-ch6という名前のコンポーザーを作成している。

DAGの実行時に必要となる環境変数PROJECTI_Dをgcpbook-ch6（先ほど作成した。）に設定
$ gcloud composer environments update gcpbook-ch6 \
--location us-central1 \
--update-env-variables=PROJECT_ID=$(gcloud config get-value project)


GCPのCloud ComposerにてAirflowのAirflowのWeb UIを起動する。
DAGの作成を行う。
→count_users.pyで作成

pyファイルを作成後以下のコマンドを実行し、Composerの環境のDAGフォルダへアップロードしている。
$ gcloud composer environments storage dags import \
--environment gcpbook-ch6 --location us-central1 \
--source count_users.py

これを実行したあとに確認するとコンソールのDAGのタブのところでアップされているのが確認する事ができる。
またAirflowUIにおいて確認してみてもおいてある事が確認する事ができる。

作成を行ったDAGを実行する。
$ gcloud composer environments run gcpbook-ch6 \
--location us-central1 \
backfill \
-- \
-s 2018-10-01 \
-e 2018-10-01 \
count_users
